{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOezSLXh9zjPyN1q9oKvv4A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvEsGLkm36Pm","executionInfo":{"status":"ok","timestamp":1718454729774,"user_tz":240,"elapsed":3,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}},"outputId":"929e2dfa-cfbe-4ab3-d21c-b0948c5440db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config', 'sample_data']"]},"metadata":{},"execution_count":1}],"source":["import os\n","# check if session has spark\n","os.listdir('/content/')\n","#os.listdir()"]},{"cell_type":"code","source":["# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n","!tar xf spark-3.5.1-bin-hadoop3.tgz\n","\n","!pip install -q findspark\n","!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uC7pSteJ371l","executionInfo":{"status":"ok","timestamp":1718454811609,"user_tz":240,"elapsed":81837,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}},"outputId":"e01918ba-3821-4cd7-86f1-1faf601c00cf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,910 kB]\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,183 kB]\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,469 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,542 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,391 kB]\n","Fetched 10.8 MB in 3s (3,654 kB/s)\n","Reading package lists... Done\n","Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b709c0436bf5bdaa295281e8faa53fdfd2a5fc3439e72fac48bf9836f3009cc9\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n"]}]},{"cell_type":"code","source":["# one time thing\n","from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHlEpJmR3-Ck","executionInfo":{"status":"ok","timestamp":1718455058235,"user_tz":240,"elapsed":1873,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}},"outputId":"b5d6b106-226c-4b33-a74b-5662a5642e7c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Set environment variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n","\n","# Start a Spark session\n","import findspark\n","findspark.init()\n","\n","# Create a SparkSession\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","#sc = SparkContext.getOrCreate()\n","spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n","spark.sparkContext.setCheckpointDir(\"/content/drive/My Drive/USPS/spark_checkpoint\")\n","\n","os.chdir('/content/drive/My Drive/USPS')\n","\n"],"metadata":{"id":"22m1YoIo4AHz","executionInfo":{"status":"ok","timestamp":1718455062635,"user_tz":240,"elapsed":4402,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Read parquet file to DF\n","folder1 = \"/content/drive/My Drive/USPS/parq_files/usps_0501.parquet\"\n","folder2 = \"/content/drive/My Drive/USPS/parq_files/usps_0502.parquet\"\n","folder3 = \"/content/drive/My Drive/USPS/parq_files/usps_0503.parquet\"\n","parc_folders = [folder1,folder2,folder3]\n","df = spark.read.parquet(*parc_folders)\n","\n","#Cleaning -----\n","# data from 0503 read ServiceType code as int so it removed 0s to the left;adding those back\n","df = df.withColumn('ServiceTypeCode', lpad(col('ServiceTypeCode'), 3, '0'))\n","\n","# Get the list of columns that contain 'zipcode'\n","zipcode_columns = [col for col in df.columns if 'zipcode' in col]\n","\n","# Define a function to add leading zeros\n","def add_leading_zeros(col):\n","    return lpad(col, 5, '0')\n","\n","# Apply the function to all zipcode columns\n","for col in zipcode_columns:\n","    df = df.withColumn(col, add_leading_zeros(df[col]))\n","\n","# recode CMRA flag\n","df = df.withColumn(\n","    'CRID_cmra_flag',\n","    when(df['CRID_cmra_flag'] == 'Y', True)\n","    .when(df['CRID_cmra_flag'].isin(['N', 'X']), False)\n","    .otherwise(None)\n",")\n","\n","# recode delinked nulls\n","df = df.withColumn(\"delinked\", when(df[\"delinked\"].isNull(), False).otherwise(df[\"delinked\"]))\n","\n"],"metadata":{"id":"VJq0hvAf4CVK","executionInfo":{"status":"ok","timestamp":1718455072028,"user_tz":240,"elapsed":9404,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# create potential fraud column\n","df = df.withColumn(\n","    \"potential_fraud\",\n","    when(df[\"delinked\"] == True, True)\n","    .when(df[\"icr_or_manifest\"] == False, True)\n","    .when(df[\"symbology_mismatch\"] == True, True)\n","    .when(df[\"firstzip_startzip_diff\"] == True, True)\n","    .otherwise(False)\n",")"],"metadata":{"id":"RP7_j6fI4QK2","executionInfo":{"status":"ok","timestamp":1718455072028,"user_tz":240,"elapsed":15,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["total_rows = df.count()\n","total_rows #61,256,141"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61tjGrt54Wzs","executionInfo":{"status":"ok","timestamp":1718455206669,"user_tz":240,"elapsed":134654,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}},"outputId":"1ec2187d-f38d-49ce-a4e6-f23fc64d51a6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["61256141"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["df.groupby('potential_fraud').count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHGrACGW4maA","executionInfo":{"status":"ok","timestamp":1718455360650,"user_tz":240,"elapsed":153984,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}},"outputId":"f75df1fc-d16f-46fa-b340-6343c865b1d6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------+--------+\n","|potential_fraud|   count|\n","+---------------+--------+\n","|           true|21918680|\n","|          false|39337461|\n","+---------------+--------+\n","\n"]}]},{"cell_type":"code","source":["n_non_fraud = 39337461"],"metadata":{"id":"RUnEhm0-RWuW","executionInfo":{"status":"ok","timestamp":1718461232630,"user_tz":240,"elapsed":328,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Select records with \"potential fraud\" == True\n","fraud_records = df.filter(df[\"potential_fraud\"] == True)\n","n_fraud = fraud_records.count()\n","# Select a random assortment of records with \"potential fraud\" == False\n","non_fraud_records = df.filter(df[\"potential_fraud\"] == False).sample(fraction = n_fraud/n_non_fraud)# total of non_fraud\n","\n","# Combine the two sets of records\n","working_dataset = fraud_records.union(non_fraud_records)"],"metadata":{"id":"hoWFm5bR4ofx","executionInfo":{"status":"ok","timestamp":1718461240463,"user_tz":240,"elapsed":6698,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["working_dataset = working_dataset.coalesce(1)"],"metadata":{"id":"Wp6wbQlJ4rbC","executionInfo":{"status":"ok","timestamp":1718461241571,"user_tz":240,"elapsed":2,"user":{"displayName":"Rosa Prieto","userId":"11930815821174800710"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["working_dataset.write.csv(\"/content/drive/My Drive/USPS/working_dataset.csv\")"],"metadata":{"id":"CCtVRGe7Tjzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_100k_noW= df.sample(fraction=0.1)\n","subset_100k_noW = subset_100k_noW.coalesce(1)\n","subset_100k_noW.write.csv(\"/content/drive/My Drive/USPS/subset_100k_noW.csv\")"],"metadata":{"id":"R-5ZFjTITp_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_10perc_W= working_dataset.sample(fraction=0.1)\n","subset_10perc_W = subset_10perc_W.coalesce(1)\n","subset_10perc_W.write.csv(\"/content/drive/My Drive/USPS/subset_10perc_W.csv\")"],"metadata":{"id":"Do2CZe57T6-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subset_1perc_W= working_dataset.sample(fraction=0.01)\n","subset_1perc_W = subset_1perc_W.coalesce(1)\n","subset_1perc_W.write.csv(\"/content/drive/My Drive/USPS/subset_1perc_W.csv\")"],"metadata":{"id":"vYU8dGzXWfbH"},"execution_count":null,"outputs":[]}]}