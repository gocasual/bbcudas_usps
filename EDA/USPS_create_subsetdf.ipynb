{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvEsGLkm36Pm",
        "outputId": "ea37fb82-9c59-47c2-878b-1d09e5148cc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "# check if session has spark\n",
        "os.listdir('/content/')\n",
        "#os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC7pSteJ371l",
        "outputId": "8ce6f79f-8d3a-40b0-893f-1a7678b2d099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [929 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,922 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,189 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,556 kB]\n",
            "Fetched 7,859 kB in 7s (1,117 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=5a6618c8fdfe82b5f87ec5420db76a9cfa620521c7606e7d81bfa38381f795d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one time thing\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHlEpJmR3-Ck",
        "outputId": "9393f646-26b9-4f99-de7d-e620dd3b5a48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "# Start a Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a SparkSession\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "#sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "spark.sparkContext.setCheckpointDir(\"/content/drive/My Drive/USPS/spark_checkpoint\")\n",
        "\n",
        "os.chdir('/content/drive/My Drive/USPS')\n",
        "\n"
      ],
      "metadata": {
        "id": "22m1YoIo4AHz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read parquet file to DF\n",
        "'''\n",
        "TODO: need to refactor to connect to the shared drive.\n",
        "'''\n",
        "folder1 = \"/content/drive/My Drive/USPS/parq_files/usps_0501.parquet\"\n",
        "folder2 = \"/content/drive/My Drive/USPS/parq_files/usps_0502.parquet\"\n",
        "folder3 = \"/content/drive/My Drive/USPS/parq_files/usps_0503.parquet\"\n",
        "parc_folders = [folder1,folder2,folder3]\n",
        "df = spark.read.parquet(*parc_folders)\n",
        "\n",
        "#Cleaning -----\n",
        "# data from 0503 read ServiceType code as int so it removed 0s to the left;adding those back\n",
        "df = df.withColumn('ServiceTypeCode', lpad(col('ServiceTypeCode'), 3, '0'))\n",
        "\n",
        "# Get the list of columns that contain 'zipcode'\n",
        "zipcode_columns = [col for col in df.columns if 'zipcode' in col]\n",
        "\n",
        "# Define a function to add leading zeros\n",
        "def add_leading_zeros(col):\n",
        "    return lpad(col, 5, '0')\n",
        "\n",
        "# Apply the function to all zipcode columns\n",
        "for col in zipcode_columns:\n",
        "    df = df.withColumn(col, add_leading_zeros(df[col]))\n",
        "\n",
        "# recode CMRA flag\n",
        "df = df.withColumn(\n",
        "    'CRID_cmra_flag',\n",
        "    when(df['CRID_cmra_flag'] == 'Y', True)\n",
        "    .when(df['CRID_cmra_flag'].isin(['N', 'X']), False)\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "# recode delinked nulls\n",
        "df = df.withColumn(\"delinked\", when(df[\"delinked\"].isNull(), False).otherwise(df[\"delinked\"]))\n",
        "\n"
      ],
      "metadata": {
        "id": "VJq0hvAf4CVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create potential fraud column\n",
        "df = df.withColumn(\n",
        "    \"potential_fraud\",\n",
        "    when(df[\"delinked\"] == True, True)\n",
        "    .when(df[\"icr_or_manifest\"] == False, True)\n",
        "    .when(df[\"symbology_mismatch\"] == True, True)\n",
        "    .when(df[\"firstzip_startzip_diff\"] == True, True)\n",
        "    .otherwise(False)\n",
        ")"
      ],
      "metadata": {
        "id": "RP7_j6fI4QK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = df.count()\n",
        "total_rows #61,256,141"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61tjGrt54Wzs",
        "outputId": "1ec2187d-f38d-49ce-a4e6-f23fc64d51a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61256141"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('potential_fraud').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHGrACGW4maA",
        "outputId": "f75df1fc-d16f-46fa-b340-6343c865b1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+\n",
            "|potential_fraud|   count|\n",
            "+---------------+--------+\n",
            "|           true|21918680|\n",
            "|          false|39337461|\n",
            "+---------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_non_fraud = 39337461"
      ],
      "metadata": {
        "id": "RUnEhm0-RWuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select records with \"potential fraud\" == True\n",
        "fraud_records = df.filter(df[\"potential_fraud\"] == True)\n",
        "n_fraud = fraud_records.count()\n",
        "# Select a random assortment of records with \"potential fraud\" == False\n",
        "non_fraud_records = df.filter(df[\"potential_fraud\"] == False).sample(fraction = n_fraud/n_non_fraud)# total of non_fraud\n",
        "\n",
        "# Combine the two sets of records\n",
        "working_dataset = fraud_records.union(non_fraud_records)"
      ],
      "metadata": {
        "id": "hoWFm5bR4ofx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working_dataset = working_dataset.coalesce(1)"
      ],
      "metadata": {
        "id": "Wp6wbQlJ4rbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working_dataset.write.csv(\"/content/drive/My Drive/USPS/working_dataset.csv\")"
      ],
      "metadata": {
        "id": "CCtVRGe7Tjzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_100k_noW= df.sample(fraction=0.1)\n",
        "subset_100k_noW = subset_100k_noW.coalesce(1)\n",
        "subset_100k_noW.write.csv(\"/content/drive/My Drive/USPS/subset_100k_noW.csv\")"
      ],
      "metadata": {
        "id": "R-5ZFjTITp_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_10perc_W= working_dataset.sample(fraction=0.1)\n",
        "subset_10perc_W = subset_10perc_W.coalesce(1)\n",
        "subset_10perc_W.write.csv(\"/content/drive/My Drive/USPS/subset_10perc_W.csv\")"
      ],
      "metadata": {
        "id": "Do2CZe57T6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_1perc_W= working_dataset.sample(fraction=0.01)\n",
        "subset_1perc_W = subset_1perc_W.coalesce(1)\n",
        "subset_1perc_W.write.csv(\"/content/drive/My Drive/USPS/subset_1perc_W.csv\")"
      ],
      "metadata": {
        "id": "vYU8dGzXWfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "0: Rosa, please add the parquet files to the shared google drive.\n",
        "1: Rosa, change google drive connection path to connect to the share drive so everyone can access\n",
        "2: Matt add your code to integrate the labels (intercepted MIDs) to the data set\n",
        "3: Rosa, upload working data csv files to google drive.\n",
        "4: Matt, after you add your code to this notebook, delete your notebook from the repo."
      ],
      "metadata": {
        "id": "yqefijQVa_Do"
      }
    }
  ]
}